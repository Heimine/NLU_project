{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jTvSTCnhtGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from albert import create_pretraining_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44dGcHothtGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check current working directory\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzvruD93htGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to create a raw text file to put all documents together and then \n",
        "# use function in albert to create pretraining data\n",
        "\n",
        "txtf = open(\"raw_data.txt\",\"w+\")\n",
        "# Change data path to your data path\n",
        "data_path = \"CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/\"\n",
        "# Currently this is only for the folder biorxiv, change to other folder is simple\n",
        "for f in os.listdir(data_path):\n",
        "    with open(data_path+f) as data:\n",
        "        samp = json.load(data)\n",
        "        # Split sentences (one sentence per line) and give a line break for each section\n",
        "        for section in samp[\"body_text\"]:\n",
        "            sentences = section[\"text\"].split(\".\")\n",
        "            for sen in sentences:\n",
        "                if sen.strip():\n",
        "                    txtf.write(sen.strip() + \".\\n\")\n",
        "                else:\n",
        "                    txtf.write(\"\\n\") # Section line break\n",
        "        break # Add this break to try a sample, if it works, delete this break\n",
        "txtf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMyDfLlHhtGu",
        "colab_type": "code",
        "colab": {},
        "outputId": "7da03f9d-0410-4227-a8fb-feea72d4bab3"
      },
      "source": [
        "# Create vocab for our model\n",
        "# See this page: https://github.com/kwonmha/bert-vocab-builder\n",
        "\n",
        "! python ./bert-vocab-builder/subword_builder.py \\\n",
        "--corpus_filepattern \"raw_data.txt\" \\\n",
        "--output_filename \"vocab.txt\" \\\n",
        "--min_count 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From ./bert-vocab-builder/subword_builder.py:81: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/tokenizer.py:133: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0411 19:05:29.006570 4661882304 module_wrapper.py:139] From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/tokenizer.py:133: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "['./raw_data.txt']\n",
            "WARNING:tensorflow:From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/tokenizer.py:138: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0411 19:05:29.007318 4661882304 module_wrapper.py:139] From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/tokenizer.py:138: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "read 100000 lines, 1.9396119117736816 secs elapsed\n",
            "read 200000 lines, 3.974220037460327 secs elapsed\n",
            "read 300000 lines, 5.845705986022949 secs elapsed\n",
            "6.597811937332153 for reading read file : ./raw_data.txt\n",
            "read all files\n",
            "WARNING:tensorflow:From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/text_encoder.py:588: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0411 19:05:35.644492 4661882304 module_wrapper.py:139] From /Users/heimine/Documents/NYU_2020_Spring/DS_GA_1012/Project/bert-vocab-builder/text_encoder.py:588: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Iteration 0\n",
            "I0411 19:05:35.644649 4661882304 text_encoder.py:588] Iteration 0\n",
            "INFO:tensorflow:vocab_size = 642884\n",
            "I0411 19:05:48.519853 4661882304 text_encoder.py:660] vocab_size = 642884\n",
            "INFO:tensorflow:Iteration 1\n",
            "I0411 19:05:48.520026 4661882304 text_encoder.py:588] Iteration 1\n",
            "INFO:tensorflow:vocab_size = 144204\n",
            "I0411 19:05:51.304255 4661882304 text_encoder.py:660] vocab_size = 144204\n",
            "INFO:tensorflow:Iteration 2\n",
            "I0411 19:05:51.304432 4661882304 text_encoder.py:588] Iteration 2\n",
            "INFO:tensorflow:vocab_size = 144204\n",
            "I0411 19:05:53.663008 4661882304 text_encoder.py:660] vocab_size = 144204\n",
            "INFO:tensorflow:Iteration 3\n",
            "I0411 19:05:53.663177 4661882304 text_encoder.py:588] Iteration 3\n",
            "INFO:tensorflow:vocab_size = 144204\n",
            "I0411 19:05:56.142684 4661882304 text_encoder.py:660] vocab_size = 144204\n",
            "INFO:tensorflow:Iteration 4\n",
            "I0411 19:05:56.142855 4661882304 text_encoder.py:588] Iteration 4\n",
            "INFO:tensorflow:vocab_size = 144204\n",
            "I0411 19:05:58.492091 4661882304 text_encoder.py:660] vocab_size = 144204\n",
            "total vocab size : 144318, 23.289731979370117 seconds elapsed \n",
            "INFO:tensorflow:vocab_size = 144318\n",
            "I0411 19:05:58.953619 4661882304 text_encoder.py:686] vocab_size = 144318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uakNo5yUhtGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m albert.create_pretraining_data \\\n",
        "  --input_file \"raw_data.txt\" \\\n",
        "  --output_file \"pre_train_data\" \\\n",
        "  --vocab_file \"vocab.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RHZZ9gwhtG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "31565f84-340a-4b41-bf83-8413427c1290"
      },
      "source": [
        "# Side note\n",
        "\n",
        "# I think it's better for us to do everything on colab...\n",
        "# To download the dataset\n",
        "# Do:\n",
        "\n",
        "! pip install kaggle\n",
        "\n",
        "# Need to get a kaggle API for this to work fine\n",
        "# FYI, see https://github.com/Kaggle/kaggle-api\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp /content/gdrive/My\\ Drive/kaggle.json ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download allen-institute-for-ai/CORD-19-research-challenge"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Downloading CORD-19-research-challenge.zip to /content\n",
            "100% 1.47G/1.47G [00:34<00:00, 53.1MB/s]\n",
            "100% 1.47G/1.47G [00:34<00:00, 45.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmjPtGpBh8Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}